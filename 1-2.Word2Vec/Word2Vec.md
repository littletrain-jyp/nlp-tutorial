# Word2Vec

CBOW: 用两边的词预测中间的词，学习速度更快，能够很好地表示高频词
skip-gram: 用中间的词来预测两边的词，在处理少量数据集的时候效果很好，能够较好的表示低频词，

Word2Vec 两种加速方法：
- 层次softmax:
  - 构造一棵哈夫曼树
  - 求最大化对数似然函数
    - 输入层：上下文词的词向量
    - 投影层：对输入向量求和或者求平均
    - 输出层：输出最有可能的词。
    - 沿着哈夫曼树找到对应的词，每一次选择就是一个二分类的过程。假设词表中有V个词，最终输出的类别是V个类别，导致最后softmax 层需要非常大的算力。因此采用分层softmax形式，只需要log(v)次。
- 负采样：
  - 统计每个词出现的概率
  - 构造正负样本
    - 正样本： 中心词+周围词
    - 负样本： 中心词+词表中随机选取的词（对于高频词，被选为负样本的概率就应该大，对于低频词，被选中的概率就小）
- 核心思想：将多分类转化为一个二分类，利用负采样后的输出分布来模拟真实的输出分布层次。


word2vec 中两个关键的超参数：
- 窗口大小
  - 采用较小的窗口（2~15）得到的嵌入： 两个嵌入之间的高相似性得分表明单词是可互换的。（只看附近距离很近的单词，反义词通常可以互换）
  - 使用较大的窗口大小（15~50）,会得到相似性更能指示单词相关性的嵌入。gensim默认窗口size是5，即前后各2个
- 负样本数量
  - 原始论文中认为5-20个负样本比较理想，不过如果数据集足够大，2-5个就足够了，gensim默认5个负样本。

# 相关学习资料
- [图解Word2vec，读这一篇就够了](https://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&mid=2651669277&idx=2&sn=bc8f0590f9e340c1f1359982726c5a30&chksm=bd4c648e8a3bed9817f30c5a512e79fe0cc6fbc58544f97c857c30b120e76508fef37cae49bc&scene=0&xtrack=1#rd)
- [小白看Word2Vec的正确打开姿势|全部理解和应用](https://zhuanlan.zhihu.com/p/120148300)
- [深入浅出Word2Vec原理解析](https://zhuanlan.zhihu.com/p/114538417)
